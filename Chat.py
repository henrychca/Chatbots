import os
import warnings
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

warnings.filterwarnings("ignore")

# ğŸ” Cargar variables desde el archivo .env
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("âŒ No se encontrÃ³ OPENAI_API_KEY en el archivo .env")

# ğŸ¤– ConfiguraciÃ³n del modelo OpenRouter
llm = ChatOpenAI(
    openai_api_base="https://openrouter.ai/api/v1",
    openai_api_key=os.environ["OPENAI_API_KEY"],
    model_name="mistralai/mistral-7b-instruct",
    temperature=0.7,
)

# ğŸ—¨ï¸ Bucle de chat bÃ¡sico
print("ğŸ’¬ Chatbot Mistral vÃ­a OpenRouter (escribe 'salir' para terminar)\n")

while True:
    user_input = input("ğŸ‘¤ TÃº: ")
    if user_input.lower() in ["salir", "exit", "quit"]:
        print("ğŸ‘‹ Hasta luego.")
        break

    try:
        response = llm.invoke([HumanMessage(content=user_input)])
        try:
            # Si es un AIMessage (objeto de mensaje)
            print(f"ğŸ¤– Bot: {response.content.strip()}\n")
        except AttributeError:
            # Si es un dict o lista de mensajes (caso nuevo en langchain_openai)
            if isinstance(response, dict) and "content" in response:
                print(f"ğŸ¤– Bot: {response['content'].strip()}\n")
            elif isinstance(response, list) and len(response) > 0:
                print(f"ğŸ¤– Bot: {response[0].content.strip()}\n")
            else:
                print(f"ğŸ¤– Bot: {response}\n")

    except Exception as e:
        print(f"âŒ Error: {e}\n")
